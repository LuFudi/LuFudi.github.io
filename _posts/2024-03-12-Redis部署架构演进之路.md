---
layout: post
title: 'Redis部署架构演化之路'
date: 2024-04-18
author: 陆福迪
cover: 'https://images.unsplash.com/photo-1739382446039-82818f664c1e?q=80&w=1933&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'
cover_author: 'Lerone Pieters'
cover_author_link: 'https://unsplash.com/@leronep'
pin: true
tags: Redis
---

### 前言

如今在项目中使用Redis越来越多，从最基础的单机到几十个节点的集群，我们应当尝试去了解，Redis 到底是如何稳定、高性能地提供服务的，更应该主动去思考：

- 业务场景很简单，只使用单机版 Redis 会有什么问题？
- 我的 Redis 故障宕机了，数据丢失了怎么办？
- 如何能保证我的业务应用不受影响？为什么需要主从集群？它有什么优势？
- 什么是分片集群？什么情况下需要分片集群？
- ...

Redis的概念也很多，如：「**数据持久化、主从复制、哨兵、分片集群**」这些，它们之间又有什么区别和联系呢？

本篇文章，我会尝试一步步构建出一个稳定、高性能的 Redis 集群。且让每一步都简单易懂

### 单机版Redis

从最简单的场景开始：

假设现在我们有一个业务应用，需要引入 Redis 来提高应用的性能，此时可以选择部署一个单机版的 Redis 来使用，就像这样：

<img src="../picture/blog/Redis/Redis-1.svg" alt="单机版" style="width: 60%; height: auto;" />

Redis作为MySQL的数据缓存，将一些频繁被读取的数据从MySQL取出后，放入Redis中。Redis的所有数据都放入内存中，他将极大的减少数据的响应时间，让我们的接口性能大大提高。

随着业务的发展，数据体量逐渐变大，Redis中的数据也越来越多，对其的依赖也逐渐增强。

直到有一天，Redis突然发生**宕机**，MySQL的**盾**突然丢失，此时原本应该由Redis挡下的刀全部直刺MySQL，巨大的流量会导致MySQL的压力剧增，导致数据查询速度下降，甚至服务不可用。

此时第一要务是什么？

当然是**重启Redis**了。但是重启后会发现一个问题：由于Redis所有数据是保存在内存中，重启后数据全部丢失，若想再次使用缓存，需要**先从MySQL读出数据再写入Redis**。所以业务流量还是都会打到后端 MySQL 上，MySQL 的压力还是很大。

那有什么方案可以解决这个问题呢？

把数据复制一份保存在磁盘不就好了

如果采用这种方式，当 Redis 重启时，我们把磁盘中的数据快速「**读取**」到内存中，这样它就可以继续正常提供服务了。这就是【**数据持久化**】



### 数据持久化

####  AOF

最简单的持久化方案是什么呢？在数据写入时，先写内存，再写入磁盘，磁盘写入成功后，返回响应。

但是这会导致响应时间显著增长，Redis将数据保存在内存中的原因，就是内存空间的高速读写，写入磁盘是一个相当耗时的操作。所以为了性能考虑，更好的方案是使用【**异步**】的方式进行磁盘的写入。

这种持久化思路，就是Redis中的AOF（Append Only File）。

Redis AOF 持久化提供了 3 种刷盘机制：

- appendfsync always：主线程同步 fsync
- appendfsync no：由 OS fsync
- appendfsync everysec：后台线程每间隔1秒 fsync

他们之间具体的区别，可以单独去搜索，这里不再赘述

但是AOF有一个缺点，它的持久化内容是操作的原始命令（如 `SET`、`DEL`、`INCR` 等），这些命令会按照执行的顺序追加到 AOF 文件中，且在服务运行时，单个key会多次被修改，每次修改的命令都会被记录下来，这导致了两个问题：

1. 持久化文件体积过大
2. 恢复时时间会比较长

针对这个问题，最先想到的解决方案肯定：是对单个key的修改，只保留最后一次操作的命令，这就是Redis提供的 AOF rewrite 方案，俗称 AOF 「瘦身」，顾名思义，就是压缩 AOF 的体积。

在 AOF 体积越来越大时（超过设定阈值），Redis 就会定期重写一份新的 AOF，这个新的 AOF 只记录数据的最终版本就可以了。

#### RDB

除了AOF，我们还有另外一种方式进行持久化。

想象一下，如果把Redis中的缓存数据比作一副画，AOF 就是记录你画画时的笔迹，根据它去重新作画，就需要从一张白纸开始按照原本的笔迹把这个画完成。

但是拿一个相机给这个画拍一张照片，拍照的这一瞬间，照片中记录到的就是画的全部细节。

**也就是说，Redis 的数据快照，是记录某一时刻下 Redis内存 中的数据，然后只需要把这个数据快照写到磁盘上就可以了。**

基于这个方案，我们可以「**定时**」给 Redis 做数据快照，把数据持久化到磁盘上。

这种方案就是我们经常听到的 Redis RDB，RDB 采用「**定时快照**」的方式进行数据持久化，

RDB的特点:

- 持久化后的文件体积更小，恢复更快
- 定时写入，且两次间隔较大

由于两次持久化之间间隔的时间比较长，即便是写操作非常频繁地情况下，RDB的间隔也不会小于1分钟（毕竟每次RDB备份都是保存**全量**的数据，这本身就是一个想当耗费资源的操作）

在这个时间段内如果 Redis 服务意外终止的话，就会造成最新的数据全部丢失。所以其适合的场景是：Redis中保存的都是缓存数据，部分数据丢失后可以从数据库再次读取

也就是说：

> 对数据完整性要求较高，选择AOF

> 对数据丢失不敏感，选择RDB

#### 混合持久化

> 混合持久化要求Redis的版本要高于4.0

再回顾一下RDB和AOF的特点：

- RDB以二进制+压缩的方式存储，文件小
- AOF可以记录每一次的修改，数据全

把他们二者综合起来，结合共同优势，就是【**混合持久化**】

具体的执行流程就是：当 AOF 在做 rewrite 时，**先将已有的历史数据在 AOF 文件中写入一个RDB快照，再把后续的数据变更操作，记录为AOF格式。**

因为 RDB 是二进制压缩写入的，这样 AOF 文件体积就变得更小了，当Redis重启时，向内存恢复数据的时间也会相应缩短。



### 主从复制

使用混合持久化，已经可以把持久化的文件优化到了最小，但是还有一个问题：宕机之后再次重启是需要时间的，在重启的过程中，我们的服务依然不可用。

那假如我们为Redis多增加两个实例，在数据写入时，将数据也同步到其他实例上，当一个实例宕机时，随机选一个正常运行实例就能直接使用了。

这种思路就是Redis的【**主从复制**】

<img src="../picture/blog/Redis/Redis-2.svg" alt="主从架构" style="width: 70%; height: auto;" />

使用主从架构会有以下好处：

1. master节点宕机后，可以快速切换到slave
2. slave可以分担master的读请求，提高服务整体吞吐量

美中不足的问题是，按照目前的架构，在master宕机后，需要【**手动**】变更slave节点为master，同时还需要通知上游那些连接 Redis 主节点的客户端，将其配置中的主节点 IP 地址更新为「新主节点」的 IP 地址。

**一切需要人工干预才能运行的架构都不是好架构**。

所以，有没有一套可以帮助我们实现【节点监控】、【故障转移】的架构呢？

有的，这就是哨兵



### 哨兵

哨兵会每隔一段时间去向master发送请求，若master超时未响应，说明该节点掉线。

但这里有一个问题，如何确定是master未回复，还是由于网络原因，该响应丢失了呢？

**搭建多个哨兵**

<img src="../picture/blog/Redis/Redis-3.svg" alt="哨兵架构" style="width: 70%; height: auto;" />

多个哨兵（默认是3个）部署在不同的服务器上，哨兵会创建一个信道，用于相互传递消息。当其中一个哨兵发现主节点超时回复时，他会认为该节点已下线。然后会发起一次所有哨兵都参与的投票，当有超过半数的哨兵都认为master掉线，就会执行主从切换，选出与主节点同步程度最高的从节点，将其设置为新的主节点，并更新所有从节点的配置，让他们去复制新的 Master。

**通知**

同时，每个连接哨兵的客户端，都会去订阅哨兵的节点变更频道，当主节点变更完成后，所有哨兵都会在该频道发布主节点切换信息，此时客户端监听到变化，就会去连接新的主节点。



在从单机到哨兵的进化中，我们也一步步的解决了数据持久化、读写分离、节点监控、故障转移的问题。哨兵模式在大部分场景下都是非常够用的，扩容从节点也非常方便，在读取缓存这块的性能来讲是没有瓶颈的。

但是还剩下一个问题，当我们服务使用的人越来越多，项目中的功能越来越复杂，Redis中缓存的数据也就会越来越多，这时候哨兵模式就会力不从心，为什么呢？因为哨兵模式下每个节点存放的都是**全量**的缓存数据，且节点要把这些数据加载到内存中，但是普通服务器的内存是有极限的。当我们的数据总量超过50G，甚至上百G时，单台服务器的内存完全无法承载这些数据。所以我们要把这些数据想办法摊到多个节点上，让他们作为一个整体对外提供服务。这就是我们要讲的集群架构。



### 分片集群

实现最简单的集群是什么呢？

<img src="../picture/blog/Redis/Redis-4.svg" alt="简单集群" style="width: 70%; height: auto;" />

搭建多套主从节点，客户端维护一套**路由规则**，在执行缓存操作之前基于该规则路由到对应的节点。但是这就有一个弊端：每个客户端都需要维护多节点Redis的配置和详细的路由规则，增加了项目的复杂度。

<img src="../picture/blog/Redis/Redis-5.svg" alt="统一代理集群" style="width: 70%; height: auto;" />

所以后续就出现了proxy代理层，将节点信息和路由规则维护在proxy中，在Redis Cluster还不成熟之前，很多开源组件集群工具都是这个思路，如Twemproxy、Codis ，这种方案的优点在于，客户端无需关心数据转发规则，只需要和 Proxy 打交道，客户端像操作单机 Redis 那样去操作后面的集群，简单易用。



<img src="../picture/blog/Redis/Redis-6.svg" alt="cluster集群" style="width: 70%; height: auto;" />

Redis Cluster可以看成多个主从架构组合起来的，每一个主从架构可以看成一个节点（其中，只有master节点具有处理请求的能力，slave节点主要是用于节点的高可用）,其**无需部署哨兵集群，集群内 Redis 节点通过 Gossip 协议互相探测健康状态，在故障时可发起自动切换。**

且Cluster提供了「配套」的 SDK，只要客户端集成 SDK，就可以连接Redis Cluster ，SDK 会自动找到 key 对应的 Redis 节点进行读写，还能自动适配 Redis 节点的增加和删除，业务侧无感知。



### 终章

从单机到持久化，到主从、哨兵、再到最终的集群，每一个架构的升级都是解决了实实在在的客观问题。随着架构复杂度的上升，其性能、吞吐量、自管性也是随之提高。

以上内容既是Redis架构演进的流程，限于篇幅原因，对于每个架构的详细运行机制并未深入探讨，可作为Redis的学习大纲使用。希望本篇文章可以为阅读者带来一些收获！



